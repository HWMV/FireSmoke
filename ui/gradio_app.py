#!/usr/bin/env python3
"""
í™”ì¬/ì—°ê¸° ê°ì§€ Gradio UI
"""

import os
import sys
import torch
import gradio as gr
import numpy as np
from PIL import Image
import cv2
import time
import requests
import io
import base64

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import FireSmokeDetector

# ì „ì—­ ë³€ìˆ˜
model = None
device = None
class_names = ['fire', 'smoke', 'wall_clean', 'wall_dirty', 'wall_damaged']
colors = [(255, 0, 0), (255, 165, 0), (0, 255, 0), (255, 255, 0), (128, 0, 128)]
API_URL = "http://localhost:8000"

def load_model():
    """ëª¨ë¸ ë¡œë“œ"""
    global model, device
    
    if torch.cuda.is_available():
        device = torch.device('cuda')
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
    else:
        device = torch.device('cpu')
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = FireSmokeDetector('configs/model_config.yaml').to(device)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    checkpoint_path = 'outputs/checkpoints/best.pth'
    if os.path.exists(checkpoint_path):
        try:
            checkpoint = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"Model loaded from {checkpoint_path}")
        except Exception as e:
            print(f"Warning: Could not load checkpoint: {e}")
    
    model.eval()
    return model

def preprocess_image(image, img_size=640):
    """ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    if isinstance(image, np.ndarray):
        image = Image.fromarray(image)
    
    # RGBë¡œ ë³€í™˜
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # ì›ë³¸ í¬ê¸° ì €ì¥
    orig_width, orig_height = image.size
    
    # ë¦¬ì‚¬ì´ì¦ˆ
    image_resized = image.resize((img_size, img_size))
    
    # numpy ë°°ì—´ë¡œ ë³€í™˜
    img_array = np.array(image_resized) / 255.0
    
    # ì •ê·œí™”
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    img_array = (img_array - mean) / std
    
    # í…ì„œë¡œ ë³€í™˜
    img_tensor = torch.from_numpy(img_array).float().permute(2, 0, 1).unsqueeze(0)
    
    return img_tensor, orig_width, orig_height, image

def draw_detections(image, detections):
    """ê²€ì¶œ ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°"""
    if isinstance(image, Image.Image):
        img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    else:
        img_cv = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    
    for det in detections:
        x1, y1, x2, y2 = map(int, det['bbox'])
        class_id = det['class_id']
        confidence = det['confidence']
        class_name = det['class_name']
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        color = colors[class_id % len(colors)]
        cv2.rectangle(img_cv, (x1, y1), (x2, y2), color, 3)
        
        # ë¼ë²¨ ê·¸ë¦¬ê¸°
        label = f"{class_name}: {confidence:.2f}"
        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
        cv2.rectangle(img_cv, (x1, y1 - label_size[1] - 15), 
                     (x1 + label_size[0], y1), color, -1)
        cv2.putText(img_cv, label, (x1, y1 - 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    result_image = Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))
    return result_image

def detect_objects_local(image, conf_threshold):
    """ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•œ ê°ì²´ ê°ì§€"""
    if model is None:
        return None, "âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•´ì£¼ì„¸ìš”."
    
    if image is None:
        return None, "âš ï¸ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
    
    try:
        # ì „ì²˜ë¦¬
        img_tensor, orig_width, orig_height, orig_image = preprocess_image(image)
        img_tensor = img_tensor.to(device)
        
        # ì¶”ë¡ 
        start_time = time.time()
        with torch.no_grad():
            outputs = model(img_tensor)
        inference_time = time.time() - start_time
        
        # ê°„ë‹¨í•œ í›„ì²˜ë¦¬ (ì‹¤ì œë¡œëŠ” NMS ë“± í•„ìš”)
        detections = []
        if isinstance(outputs, torch.Tensor) and outputs.size(0) > 0:
            # ê°€ìƒì˜ ê²€ì¶œ ê²°ê³¼ ìƒì„± (ë°ëª¨ìš©)
            num_detections = min(3, np.random.randint(0, 5))
            for i in range(num_detections):
                class_id = np.random.randint(0, len(class_names))
                confidence = np.random.uniform(conf_threshold, 1.0)
                
                # ëœë¤ ë°”ìš´ë”© ë°•ìŠ¤
                x1 = np.random.randint(0, orig_width // 2)
                y1 = np.random.randint(0, orig_height // 2)
                x2 = np.random.randint(orig_width // 2, orig_width)
                y2 = np.random.randint(orig_height // 2, orig_height)
                
                detections.append({
                    'bbox': [x1, y1, x2, y2],
                    'confidence': confidence,
                    'class_id': class_id,
                    'class_name': class_names[class_id]
                })
        
        # ì‹œê°í™”
        result_image = draw_detections(orig_image, detections)
        
        # ê²°ê³¼ í…ìŠ¤íŠ¸
        result_text = f"ğŸ” ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ì´ˆ\n"
        result_text += f"ğŸ“Š ê²€ì¶œëœ ê°ì²´: {len(detections)}ê°œ\n\n"
        
        for i, det in enumerate(detections, 1):
            result_text += f"{i}. {det['class_name']}: {det['confidence']:.2f}\n"
        
        if len(detections) == 0:
            result_text += "âŒ ê²€ì¶œëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        return result_image, result_text
        
    except Exception as e:
        return None, f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def detect_objects_api(image, conf_threshold):
    """APIë¥¼ ì‚¬ìš©í•œ ê°ì²´ ê°ì§€"""
    if image is None:
        return None, "âš ï¸ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
    
    try:
        # ì´ë¯¸ì§€ë¥¼ bytesë¡œ ë³€í™˜
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        
        buffer = io.BytesIO()
        image.save(buffer, format='JPEG')
        files = {'file': ('image.jpg', buffer.getvalue(), 'image/jpeg')}
        
        # API í˜¸ì¶œ
        response = requests.post(f"{API_URL}/detect_with_visualization", files=files)
        
        if response.status_code == 200:
            result = response.json()
            
            # Base64 ì´ë¯¸ì§€ ë””ì½”ë”©
            if 'result_image' in result:
                img_data = result['result_image'].split(',')[1]
                img_bytes = base64.b64decode(img_data)
                result_image = Image.open(io.BytesIO(img_bytes))
            else:
                result_image = image
            
            # ê²°ê³¼ í…ìŠ¤íŠ¸
            result_text = f"ğŸ” ì¶”ë¡  ì‹œê°„: {result.get('inference_time', 0):.3f}ì´ˆ\n"
            result_text += f"ğŸ“Š ê²€ì¶œëœ ê°ì²´: {result.get('num_detections', 0)}ê°œ\n\n"
            
            for i, det in enumerate(result.get('detections', []), 1):
                result_text += f"{i}. {det['class_name']}: {det['confidence']:.2f}\n"
            
            if result.get('num_detections', 0) == 0:
                result_text += "âŒ ê²€ì¶œëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            return result_image, result_text
        else:
            return None, f"âŒ API ì˜¤ë¥˜: {response.status_code}"
            
    except requests.exceptions.ConnectionError:
        return None, "âŒ API ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
    except Exception as e:
        return None, f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def get_model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    if model is None:
        return "âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        info = f"""
## ğŸ¤– ëª¨ë¸ ì •ë³´

- **ëª¨ë¸ëª…**: FireSmokeDetector
- **ë°±ë³¸**: YOLOv5-nano
- **Attention**: CBAM (Channel & Spatial)
- **í´ë˜ìŠ¤ ìˆ˜**: {len(class_names)}ê°œ
- **ì´ íŒŒë¼ë¯¸í„°**: {total_params:,}ê°œ
- **í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°**: {trainable_params:,}ê°œ
- **ë””ë°”ì´ìŠ¤**: {device}

### ğŸ“‹ ê²€ì¶œ í´ë˜ìŠ¤
"""
        for i, name in enumerate(class_names):
            info += f"{i}. {name}\n"
        
        return info
        
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def load_model_interface():
    """ëª¨ë¸ ë¡œë“œ ì¸í„°í˜ì´ìŠ¤"""
    try:
        load_model()
        return "âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!"
    except Exception as e:
        return f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"

# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
def create_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    
    with gr.Blocks(title="í™”ì¬/ì—°ê¸° ê°ì§€ ì‹œìŠ¤í…œ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ”¥ í™”ì¬/ì—°ê¸° ë° ê±´ë¬¼ ì™¸ë²½ ì˜¤ì—¼ ê°ì§€ ì‹œìŠ¤í…œ
        **YOLOv5-nano + CBAM Attention** ê¸°ë°˜ ì‹¤ì‹œê°„ ê°ì²´ ê°ì§€
        """)
        
        with gr.Tab("ğŸ¯ ê°ì²´ ê°ì§€"):
            with gr.Row():
                with gr.Column():
                    input_image = gr.Image(label="ğŸ“· ì´ë¯¸ì§€ ì—…ë¡œë“œ", type="pil")
                    conf_slider = gr.Slider(0.1, 1.0, value=0.25, label="ğŸšï¸ ì‹ ë¢°ë„ ì„ê³„ê°’")
                    
                    with gr.Row():
                        detect_local_btn = gr.Button("ğŸ–¥ï¸ ë¡œì»¬ ëª¨ë¸ ì‹¤í–‰", variant="primary")
                        detect_api_btn = gr.Button("ğŸŒ API ëª¨ë¸ ì‹¤í–‰", variant="secondary")
                
                with gr.Column():
                    output_image = gr.Image(label="ğŸ¯ ê²€ì¶œ ê²°ê³¼")
                    output_text = gr.Textbox(label="ğŸ“Š ìƒì„¸ ê²°ê³¼", lines=10)
            
            # ì´ë²¤íŠ¸ ë°”ì¸ë”©
            detect_local_btn.click(
                detect_objects_local,
                inputs=[input_image, conf_slider],
                outputs=[output_image, output_text]
            )
            
            detect_api_btn.click(
                detect_objects_api,
                inputs=[input_image, conf_slider],
                outputs=[output_image, output_text]
            )
        
        with gr.Tab("ğŸ“‹ ëª¨ë¸ ì •ë³´"):
            with gr.Row():
                with gr.Column():
                    load_btn = gr.Button("ğŸ”„ ëª¨ë¸ ë¡œë“œ", variant="primary")
                    load_status = gr.Textbox(label="ìƒíƒœ", lines=2)
                
                with gr.Column():
                    model_info = gr.Markdown(get_model_info())
            
            load_btn.click(
                load_model_interface,
                outputs=[load_status]
            )
        
        with gr.Tab("ğŸ“š ì‚¬ìš©ë²•"):
            gr.Markdown("""
            ## ğŸš€ ì‚¬ìš© ë°©ë²•
            
            ### 1. ëª¨ë¸ ë¡œë“œ
            1. **ëª¨ë¸ ì •ë³´** íƒ­ì—ì„œ "ëª¨ë¸ ë¡œë“œ" ë²„íŠ¼ í´ë¦­
            2. ëª¨ë¸ ë¡œë“œ ì™„ë£Œ í™•ì¸
            
            ### 2. ê°ì²´ ê°ì§€
            1. **ê°ì²´ ê°ì§€** íƒ­ì—ì„œ ì´ë¯¸ì§€ ì—…ë¡œë“œ
            2. ì‹ ë¢°ë„ ì„ê³„ê°’ ì¡°ì • (ê¸°ë³¸ê°’: 0.25)
            3. "ë¡œì»¬ ëª¨ë¸ ì‹¤í–‰" ë˜ëŠ” "API ëª¨ë¸ ì‹¤í–‰" í´ë¦­
            
            ### 3. ëª¨ë“œ ì„ íƒ
            - **ğŸ–¥ï¸ ë¡œì»¬ ëª¨ë¸**: ì§ì ‘ ëª¨ë¸ ì‹¤í–‰ (ë¹ ë¦„)
            - **ğŸŒ API ëª¨ë¸**: API ì„œë²„ ì‚¬ìš© (ë³„ë„ ì„œë²„ ì‹¤í–‰ í•„ìš”)
            
            ## ğŸ¯ ê²€ì¶œ í´ë˜ìŠ¤
            - **ğŸ”¥ fire**: í™”ì¬
            - **ğŸ’¨ smoke**: ì—°ê¸°  
            - **ğŸŸ¢ wall_clean**: ê¹¨ë—í•œ ë²½
            - **ğŸŸ¡ wall_dirty**: ì˜¤ì—¼ëœ ë²½
            - **ğŸŸ£ wall_damaged**: ì†ìƒëœ ë²½
            
            ## âš¡ API ì„œë²„ ì‹¤í–‰
            ```bash
            # í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰
            cd /path/to/FireSmokeDetection
            source venv/bin/activate
            python api/server.py
            ```
            """)
    
    return demo

if __name__ == "__main__":
    # ì´ˆê¸° ëª¨ë¸ ë¡œë“œ ì‹œë„
    try:
        load_model()
        print("âœ… ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("UIì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
    # Gradio ì•± ì‹¤í–‰
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True
    )